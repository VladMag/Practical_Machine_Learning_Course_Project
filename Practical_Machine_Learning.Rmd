---
title: "Practical Machine Learning Course Project"
author: "Vladimir Maganov"
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

```{r SettingUpTheGlobalOptions, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)              #making "echo=TRUE" default for all code chunks
```


## Synopsis
The goal of this project is to build, train and evaluate a classification model (for predicting the manner in which people did the exercise) based on data from the research paper [_"Qualitative Activity Recognition of Weight Lifting Exercises"_](https://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) (see the section on the _Weight Lifting Exercise Dataset_).


## Background
The purpose of the researchers' original experiment was to assess whether mistakes in weight-lifting exercises could be detected by using activity recognition techniques.
They recorded users performing the same activity correctly and with a set of common mistakes with wearable sensors.


## Data
According to the project instructions, we are supposed to use the following datasets (but not the original [_WLE dataset_](http://web.archive.org/web/20161224072740/http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv) from the research paper):

* https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv - as the training data
* https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv - as the test data

The `classe` variable in the __training set__ represents the observed outcome (type of activity) and, being categorical, assigns the observation to the following classes: class `A` corresponds to the correct execution of the exercise, while the other 4 classes (classes `B`, `C`, `D` and `E`) correspond to common mistakes.
This variable is missing in the __test set__ and must be predicted by our model.


## Getting and Cleaning Data
The `pml-training.csv` data set is a data frame with 19622 observations on 160 variables.  
```{r DataLoad, cache=FALSE}
rawTraining <- read.csv("pml-training.csv",header=TRUE,na.strings=c("NA","#DIV/0!",""))  # raw training data
dim(rawTraining)  # dimension of the dataset
```

### Missing Values
The dataset appears to have missing values (coded as NA) that need handling.
```{r CheckForNA, cache=FALSE}
VariablesWithNA <- apply(rawTraining,2,anyNA) ; sum(VariablesWithNA)  # list of variables with NA
summary( colMeans( is.na(rawTraining[,VariablesWithNA]) ) )  # statistics on proportion of NA's
```

100 variables contain NA's. Since the proportion of missing values in each of these variables is very high (minimum 0.9793), it does make sense to remove these features from the dataset. 
```{r RemoveNA, cache=FALSE}
cleanTraining <- rawTraining[,!VariablesWithNA] ; dim(cleanTraining)  # features with NA's removed
```


## Exploratory Data Analysis
The first six features (skipping the `user_name` variable) appear irrelevant to the outcome so we can remove them as well.  
```{r RemovingIrrelevantFeatures, cache=FALSE}
cleanTraining2 <- cleanTraining[,-c(1,3:7)] ; dim(cleanTraining2)  # irrelevant features removed
```

### Zero- and Near Zero-variance predictors
Here we examine so-called _"zero-variance"_ and _"near zero-variance"_ predictors that may cause problem for some models.  
Although in this case, there are none to worry about.
```{r LoadLibrary, warning=FALSE, message=FALSE}
library(caret) ; library(randomForest) ; library(gbm)  # load the libraries 
```
```{r ZeroVariancePredictors, cache=TRUE}
nearZeroVar(cleanTraining2)  # check for "near zero-variance" predictors
```

### Multicollinearity
Here we examine the remaining data for highly correlated predictors that some models are susceptible to.  
```{r HighlyCorrelatedPredictors, cache=FALSE}
tmpHighCorrVar <- findCorrelation( cor(cleanTraining2[,-c(1,54)]) )  # highly correlated variables
HighCorrVarNames <- names(cleanTraining2[,-c(1,54)])[tmpHighCorrVar] ; HighCorrVarNames  # variable names
```

7 variables are found to have high pairwise correlation (>0.9) and we remove them from the training dataset.
```{r RemovingHighlyCorrelatedPredictors, cache=FALSE}
cleanTraining3 <- within(cleanTraining2, rm(list=HighCorrVarNames))  # correlated features removed
dim(cleanTraining3)  # dimension of the final training dataset
```


## Model Selection and Building
Since we must predict a qualitative (categorical) response, it makes sense to apply and compare several classification techniques covered in the class e.g. Random Forests (method `rf`), Boosting (method `gbm`) and Linear Discriminant Analysis (method `lda`).  
For estimating the test (_out of sample_) error, we use the 10-fold cross-validation approach with the training data, and the resulting accuracy and error rate provide an optimistic estimate of model performance on a new (an independent) data set.
```{r FitModels, cache=TRUE, results="hide"}
set.seed(12345)
modRF  <- train(classe ~ . , data=cleanTraining3, method="rf", trControl=trainControl(method="cv",number=10))

set.seed(23456)
modGBM <- train(classe ~ . , data=cleanTraining3, method="gbm", trControl=trainControl(method="cv",number=10))

set.seed(34567)
modLDA <- train(classe ~ . , data=cleanTraining3, method="lda", trControl=trainControl(method="cv",number=10))
```

The following table summarizes the resampling (cross-validation) results for our models.  
```{r InterpretationOfResults}
rbind( getTrainPerf(modRF), getTrainPerf(modGBM), getTrainPerf(modLDA) )
```
Among our three candidates, the _Random Forests_ model has the best accuracy ($`r round(max(modRF$results$Accuracy),4)`$, error rate $`r 1-round(max(modRF$results$Accuracy),4)`$, see _Appendix_ for more detail) with which we can predict the response for observations not used in model training.


## Predictions
In this final section, we apply the resulting machine learning algorithm to the test dataset.
```{r PredictOnTestData}
TestData <- read.csv("pml-testing.csv",header=TRUE,na.strings=c("NA","#DIV/0!",""))  # load the test data
dim(TestData)  # dimension of the test dataset

predRF <- predict(modRF, TestData) ; predRF  # getting predictions
```

***

## Appendix

### Figure 1. Summary of the _Random Forests_ model

```{r Figure1}
print.train( modRF, printCall=TRUE, selectCol=TRUE, digits=4 )

```

### Figure 2. Resampling results
The resulting model accuracy is the average accuracy of the 10 held-out folds.
```{r Figure2}
modRF$resample
print( colMeans(modRF$resample[1:2]) , digits=4 )
```

### Figure 3. Predicting on the full training data

```{r Figure3}
confusionMatrix( predict(modRF,cleanTraining3) , as.factor(cleanTraining3$classe) )
```

### Figure 4. Variable Importance

```{r Figure4, fig.height=7}
plot(varImp(modRF))
```

***
